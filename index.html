<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Reproducible Methods for Face Research</title>

<script src="index_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="index_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="index_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<link href="index_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="index_files/highlightjs-9.12.0/highlight.js"></script>
<script src="index_files/kePrint-0.0.1/kePrint.js"></script>
<link href="index_files/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>


<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Reproducible Methods for Face Research</h1>
<h4 class="author">Lisa DeBruine, Iris Holzleitner, Bernard Tiddeman, Benedict C. Jones</h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>Face stimuli are commonly created in ways that are not explained well enough for others to reproduce them. In this paper, we document the irreproducibility of most face stimuli, explain the benefits of reproducible stimuli, and introduce the open-source R package webmorphR that facilitates scriptable face image processing. We explain the technical processes of morphing and transforming through a case study of creating face stimuli from an open-access image set. Finally, we discuss some ethical and methodological issues around the use of face images in research that may be ameliorated through the use of reproducible stimuli.</p>
</div>

</div>


<style>
  blockquote {
    font-size: 13px;
    border: 1px solid grey;
    background-color: #EEE;
    border-radius: 1em;
  }

  address { display: none; }
</style>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Face stimuli are commonly used in research on visual and social perception. Faces are thought to play a core role in social interaction, with a wealth of research on brain areas for face processing <span class="citation">(Duchaine &amp; Yovel, 2015)</span>, emotional and social information communicated by faces <span class="citation">(Jack &amp; Schyns, 2017)</span>, and the role of facial appearance in shaping stereotypes <span class="citation">(Olivola et al., 2014; Todorov et al., 2008a)</span>, to give just a few examples. This research almost always involves some level of stimulus preparation to rotate, resize, crop, and reposition faces on the image. In addition, many studies systematically manipulate face images by changing color and/or shape properties <span class="citation">(e.g., Perrett et al., 1994, 1998; Stephen et al., 2012; reviewed in Little et al., 2011)</span>.</p>
<p>Over a decade ago, Gronenschild et al. <span class="citation">(2009)</span> argued for the importance of standardizing face stimuli for “factors such as brightness and contrast, head size, hair cut and color, skin color, and the presence of glasses and earrings”. They describe a three-step standardization process. First, they manually removed features such as glasses and earrings in Photoshop. Second, they geometrically standardized images by semi-automatically defining eye and mouth coordinates used to fit the images within an oval mask, Third, they optically standardized images by converting them to greyscale and remapping values between the minimum and 98% threshold onto the full range of values. While laudable in its aims, this procedure has not achieved widespread adoption, probably because the authors provided no code or tools. In personal communication, the main author said that this is because “the procedure is based on standard image processing algorithms described in many textbooks”. However, we were unable to easily replicate the procedure and found several places where instructions had more than one possible interpretation or relied on the starting images having specific properties, such as symmetric lighting reflections in the eyes. Additionally, greyscale images with an oval mask are not appropriate for many research questions. Indeed, color information can have important effects on perception <span class="citation">(Stephen et al., 2012)</span> and the oval mask can affect perception in potentially unintended ways <span class="citation">(Hong Liu &amp; Chen, 2018)</span>.</p>
<p>The goal of this paper is to argue for the importance of reproducible stimulus processing methods in face research and to introduce an open-source R package that allows researchers to create face stimuli with scripts that can then be shared so that others can create stimuli using identical methods.</p>
<div id="why-are-reproducible-stimulus-construction-methods-important" class="section level2">
<h2>Why are reproducible stimulus construction methods important?</h2>
<p>Lisa once gave up on a research project because she couldn’t figure out how to manipulate spatial frequency to make the stimuli look like those in a relevant paper. When she contacted the author, they didn’t know how the stimuli were created because a postdoc had done it in Photoshop and didn’t leave a detailed record of the method.</p>
<p>Reproducibility is especially important for face stimuli because faces are sampled, so replications should sample new <em>faces</em> as well as new participants <span class="citation">(Barr, 2007)</span>. The difficulty of creating equivalent face stimuli is a major barrier to this, resulting in stimulus sets that are used across dozens or hundreds of papers. For example, the Chicago Face Database <span class="citation">(Ma et al., 2015)</span> has been cited in almost 800 papers. Ekman and Friesen’s <span class="citation">(1976)</span> Pictures of Facial Affect has been cited more than 5500 times. This image set is currently <a href="https://www.paulekman.com/product/pictures-of-facial-affect-pofa/"><strong>selling</strong> for $399</a> for “110 photographs of facial expressions that have been widely used in cross-cultural studies, and more recently, in neuropsychological research”. Such extensive reuse of image sets means that any confounds present in a particular image set can result in findings that are highly “replicable” but potentially just an artifact of the set-specific confounds.</p>
<p>Additionally, image sets are often private and reused without clear attribution. Our group has only recently been trying to combat this by making image sets public and citable where possible <span class="citation">(DeBruine, 2016; DeBruine &amp; Jones, 2017a; e.g., DeBruine &amp; Jones, 2017b, 2020; B. C. Jones et al., 2018; Morrison et al., 2018)</span> and including clear explanations of reuse where not possible <span class="citation">(e.g., Holzleitner et al., 2019)</span>.</p>
<!--
holzleitner2019comparing: All participants were students at the University of Glasgow, participating as part of a larger project on hormones and mating psychology (B. C. Jones, Hahn, Fisher, Wang, Kandrik, & DeBruine, 2018; B. C. Jones, Hahn, Fisher, Wang, Kandrik, Han, et al., 2018; B. C. Jones, Hahn, Fisher, Wang, Kandrik, Lee, et al., 2018).
-->
</div>
<div id="common-techniques" class="section level2">
<h2>Common Techniques</h2>
<p>In this section, we will give an overview of common techniques used to process face stimuli across a wide range of research involving faces. It was basically impossible to systematically survey the literature about the methods used to create facial stimuli, in large part because of poor documentation. However, several common methods are discussed below.</p>
<div id="vague-methods" class="section level3">
<h3>Vague Methods</h3>
<p>Many researchers describe image manipulation generically or use “in-house” methods that are not well specified enough for another researcher to have any chance of replicating them. Consider this text from <span class="citation">Burton et al. (2005)</span> (p. 263).</p>
<blockquote>
<p>Each of the images was rendered in gray-scale and morphed to a common shape using an in-house program based on bi-linear interpolation (see e.g., Gonzalez &amp; Woods, 2002). Key points in the morphing grid were set manually, using a graphics program to align a standard grid to a set of facial points (eye corners, face outline, etc.). Images were then subject to automatic histogram equalization.</p>
</blockquote>
<p>The reference to <span class="citation">Gonzalez et al. (2002)</span> is a 190-page textbook. It mentions bilinear interpolation on pages 64–66 in the context of calculating pixel color when resizing images and it’s unclear how this could be used to morph shape.</p>
<p>While the example below includes images in the mentioned figure that help to clarify the methods, it is clear that there was a large degree of subjectivity in determining how to crop the hair.</p>
<blockquote>
<p>They were cropped such that the hair did not extend well below the chin, resized to a height of 400 pixels, and placed on 400 x 400 pixel backgrounds consisting of phase-scrambled variations of a single scene image (for example stimuli, see Figure 1). <span class="citation">(Pegors et al., 2015, p. 665)</span></p>
</blockquote>
</div>
<div id="photoshopimage-editors" class="section level3">
<h3>Photoshop/Image editors</h3>
<p>A search for “Photoshop face attractiveness” produced 19,300 responses in Google Scholar<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Here are descriptions of the use of Photoshop from a few of the top hits.</p>
<blockquote>
<p>If necessary, scanned pictures were rotated slightly, using Adobe Photoshop software, clockwise to counterclockwise until both pupil centres were on the same y-coordinate. Each picture was slightly lightened a constant amount by Adobe Photoshop. <span class="citation">(Scheib et al., 1999, p. 1914)</span></p>
</blockquote>
<blockquote>
<p>These pictures were edited using Adobe Photoshop 6.0 to remove external features (hair, ears) and create a uniform grey background. <span class="citation">(Sforza et al., 2010, p. 150)</span></p>
</blockquote>
<blockquote>
<p>The averaged composites and blends were sharpened in Adobe Photoshop to reduce any blurring introduced by blending. <span class="citation">(Rhodes et al., 2001, p. 615)</span></p>
</blockquote>
<p>Most papers that use Photoshop methods simply state in lay terms what the editing accomplished, and not the specific tools or methods in the application used to accomplish it. For example, it is not clear what sharpening tool was used in the last quote above, and what settings were used. Were all images sharpened by the same amount or was this done “by eye”?</p>
<p>A potential danger to processing images “by eye” is the possibility of visual adaptation affecting the researcher’s perception. It is well known that viewing images with specific alterations to shape or colour alters the perception of subsequent images <span class="citation">(Rhodes, 2017)</span>. Thus, a researcher’s perception of the “typical” face can change after exposure to altered faces <span class="citation">(DeBruine et al., 2007; O’Neil &amp; Webster, 2011; Rhodes &amp; Leopold, 2011; Webster &amp; MacLeod, 2011)</span>. While some processing will always require human intervention, reproducible methods can also allow researchers to record their specific decisions so such biases can be detected and corrected for.</p>
</div>
<div id="scriptable-methods" class="section level3">
<h3>Scriptable Methods</h3>
<p>There are several scriptable methods for creating image stimuli, including MatLab, ImageMagick, and GraphicConvertor. Photoshop is technically scriptable, but a search of “Photoshop script face” only revealed a few computer vision papers on detecting photoshopped images <span class="citation">(e.g., Wang et al., 2019)</span>.</p>
<p>MatLab <span class="citation">(Higham &amp; Higham, 2016)</span> is widely used within visual psychophysics. A Google Scholar search for “MatLab face attractiveness” returned 23,000 hits, although the majority of papers we inspected used MatLab to process EEG data, present the experiment, or analyse image color, rather than using MatLab to create the stimuli. “MatLab face perception” generated 97,300 hits, more of which used MatLab to create stimuli.</p>
<blockquote>
<p>The average pixel intensity of each image (ranging from 0 to 255) was set to 128 with a standard deviation of 40 using the SHINE toolbox (function lumMatch) (Willenbockel et al., 2010) in MATLAB (version 8.1.0.604, R2013a). <span class="citation">(Visconti di Oleggio Castello et al., 2014, p. 2)</span></p>
</blockquote>
<p>ImageMagick <span class="citation">(The ImageMagick Development Team, 2021)</span> is a free, open-source program that creates, edits, and converts images in a scriptable manner. The {magick} R package <span class="citation">(Ooms, 2021)</span> allows you to script image manipulations in R using ImageMagick.</p>
<blockquote>
<p>Images were cropped, resized to 150 × 150 pixels, and then grayscaled using ImageMagick (version 6.8.7-7 Q16, x86_64, 2013-11-27) on Mac OS X 10.9.2. <span class="citation">(Visconti di Oleggio Castello et al., 2014, p. 2)</span></p>
</blockquote>
<p>GraphicConvertor <span class="citation">(Nishimura, 2000)</span> is typically used to batch process images, such as making images a standard size or adjusting color. While not technically “scriptable”, batch processing can be set up in the GUI interface and then saved to a reloadable “.gaction” file. (A search for ‘“gaction” GraphicConvertor’ on Google Scholar returned no hits.)</p>
<blockquote>
<p>We used the GraphicConverterTM application to crop the images around the cat face and make them all 1024x1024 pixels. One of the challenges of image matching is to do this process automatically. <span class="citation">(Paluszek &amp; Thomas, 2019, p. 214)</span></p>
</blockquote>
<p>Scriptable methods are a laudable start to reproducible stimuli, but the scripts themselves are often not shared, or are in a proprietary closed format, such as MatLab. Additionally, most images that were processed with scriptable methods also used some non-scripted pre-processing to manually crop or align the images.</p>
</div>
<div id="commerical-morphing" class="section level3">
<h3>Commerical morphing</h3>
<p>Face averaging or “morphing” is a common technique for making images that are blends of two or more faces. We found 937 Google Scholar responses for “Fantamorph face”, 170 responses for “WinMorph face” and fewer mentions of several other programs, such as MorphThing (no longer available) and xmorph.</p>
<p>Most of these programs do not use open formats for storing delineations: the x- and y-coordinates of the landmark points that define shape and the way these are connected with lines. Their algorithms also tend to be closed and there is no common language for describing the procedures used to create stimuli in one program in a way that is easily translatable to another program. Here are descriptions of the use of commercial morphing programs from a few of the top hits.</p>
<blockquote>
<p>The faces were carefully marked with 112 nodes in FantaMorph™, 4th version: 28 nodes (face outline), 16 (nose), 5 (each ear), 20 (lips), 11 (each eye), and 8 (each eyebrow). To create the prototypes, I used FantaMorph Face Mixer, which averages node locations across faces. Prototypes are available online, in the Personality Faceaurus [<a href="http://www.nickholtzman.com/faceaurus.htm" class="uri">http://www.nickholtzman.com/faceaurus.htm</a>]. <span class="citation">(Holtzman, 2011a, p. 650)</span></p>
</blockquote>
<p>The link above contains only morphed face images and no further details about the morphing or stimulus preparation procedure.</p>
<blockquote>
<p>The 20 individual stimuli of each category were paired to make 10 morph continua, by morphing one endpoint exemplar into its paired exemplar (e.g. one face into its paired face, see Figure 1C) in steps of 5%. Morphing was realized within FantaMorph Software (Abrosoft) for faces and cars, Poser 6 for bodies (only between stimuli of the same gender with same clothing), and Google SketchUp for places. <span class="citation">(Weigelt et al., 2013, p. 4)</span></p>
</blockquote>
</div>
</div>
<div id="psychomorph" class="section level2">
<h2>Psychomorph/WebMorph</h2>
<p>Psychomorph is a program developed by Benson, Perrett, Tiddeman and colleagues. It uses “template” files in a plain text open format to store delineations and the code is well documented in academic papers and available as an open-source Java package.</p>
<p>Benson and Perrett <span class="citation">(Benson &amp; Perrett, 1991a, 1991b, 1993)</span> describe algorithms for creating composite images by marking corresponding coordinates on individual face images, remapping the images into the average shape, and combining the colour values of the remapped images. These images are also called “prototype” images and can be used to generate caricatures.</p>
<p>The averaging and caricaturing methods were later complemented by a transforming method <span class="citation">(Rowland &amp; Perrett, 1995)</span>. This method quantifies shape and colour differences between a pair of faces, creating a “face space” vector along which other faces can be manipulated. This method is distinct from averaging. For example, averaging an individual face with a prototype smiling face will produce a face that looks approximately halfway between the individual and the prototype. The smile will be more intense than the original individual’s smile if they weren’t smiling, and be less intense if the individual was smiling more than the prototype. However, the transform method defines the shape and/or color difference between neutral and smiling prototypes to define a vector of smiling. Transforming an individual face by some positive percent of the difference between neutral and smiling faces will then always result in an individual face that looks <em>more</em> cheerful than the original individual, no matter how cheerful they started out (Fig <a href="#fig:transform-demo">1</a>).</p>

<div class="figure"><span style="display:block;" id="fig:transform-demo"></span>
<img src="index_files/figure-html/transform-demo-1.png" alt="Composite (A) neutral and (B) smiling faces made from 49 indvidual neutral and smiling identities. (C) Individual smiling faces were (D) averaged with the smiling composite or (E) transformed by 50% of the shape and color differences between the neutral and smiling composites (E)." width="100%" />
<p class="caption">
Figure 1: Composite (A) neutral and (B) smiling faces made from 49 indvidual neutral and smiling identities. (C) Individual smiling faces were (D) averaged with the smiling composite or (E) transformed by 50% of the shape and color differences between the neutral and smiling composites (E).
</p>
</div>
<p>These methods were improved by wavelet-based texture averaging <span class="citation">(Tiddeman et al., 2001)</span>, resulting in images with more realistic textural details, such as facial hair and eyebrows. This reduces the “fuzzy” look of composite images, but can also result in artifacts, such as lines on the forehead in Figure <a href="#fig:texture-comp">2</a>, which are a result of some images having a fringe.</p>
<div class="figure"><span style="display:block;" id="fig:texture-comp"></span>
<img src="index_files/figure-html/texture-comp-1.png" alt="Untextured and textured prototypes of 4 male faces." width="100%" />
<p class="caption">
Figure 2: Untextured and textured prototypes of 4 male faces.
</p>
</div>
<p>The desktop version of Psychomorph was last updated in 2013, and can be difficult to install on some computers. To solve this problem, we started developing WebMorph <span class="citation">(DeBruine, 2018)</span>, a web-based version that uses the <a href="https://users.aber.ac.uk/bpt/jpsychomorph/version6/javadoc/">Facemorph Java package</a> from Psychomorph for averaging and transforming images, but has independent methods for delineation and batch processing. While the desktop version of Psychomorph has limited batch processing ability, it requires a knowledge of Java to be fully scriptable. WebMorph has more extensive batch processing capacity, including the ability to set up image processing scripts in a spreadsheet, but some processes such as delineation still require a fair amount of manual processing. In this paper, we introduce webmorphR <span class="citation">(DeBruine, 2022a)</span>, an R package companion to WebMorph that allows you to create R scripts to fully and reproducibly describe all of the steps of image processing and easily apply them to a new set of images.</p>
<table class="table table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:glossary">Table 1: </span>Glossary of terms.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Term
</th>
<th style="text-align:left;">
Definition
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
composite
</td>
<td style="text-align:left;">
an average of more than one face image
</td>
</tr>
<tr>
<td style="text-align:left;">
delineation
</td>
<td style="text-align:left;">
the x- and y-coordinates for a specific template that describe an image
</td>
</tr>
<tr>
<td style="text-align:left;">
landmark
</td>
<td style="text-align:left;">
a point that marks corresponding locations on different images
</td>
</tr>
<tr>
<td style="text-align:left;">
lines
</td>
<td style="text-align:left;">
connections between landmarks; these may be used to interpolate new landmarks for morphing
</td>
</tr>
<tr>
<td style="text-align:left;">
morphing
</td>
<td style="text-align:left;">
blending two or more images to make an image with an average shape andor color
</td>
</tr>
<tr>
<td style="text-align:left;">
prototype
</td>
<td style="text-align:left;">
an average of faces with similar characteristics, such as expression, gender, age, and/or ethnic group
</td>
</tr>
<tr>
<td style="text-align:left;">
template
</td>
<td style="text-align:left;">
a set of landmark points that define shape and the way these are connected with lines; only image with the same template can be averaged or transformed
</td>
</tr>
<tr>
<td style="text-align:left;">
transforming
</td>
<td style="text-align:left;">
changing the shape and/or color of an image by some proportion of a vector that is defined as the difference between two images
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="methods" class="section level1">
<h1>Methods</h1>
<p>In this section, we will cover some common image manipulations and how to achieve them reproducibly using webmorphR <span class="citation">(DeBruine, 2022a)</span>. We will also be using webmorphR.stim <span class="citation">(DeBruine &amp; Jones, 2022)</span>, a package that contains a number of open-source face image sets, and webmorphR.dlib <span class="citation">(DeBruine, 2022b)</span>, a package that provides dlib models and functions for automatic face detection. These latter two packages cannot be made available on CRAN (the main repository for R packages) because of their large file size.</p>
<div id="editing" class="section level2">
<h2>Editing</h2>
<p>Almost all image sets start with raw images that need to be cropped, resized, rotated, padded, and/or color normalised. Although many reproducible methods exist to manipulate images in these ways, they are complicated when an image has an associated delineation, so webmorphR has functions that alter the image and delineation together (Fig. <a href="#fig:editing">3</a>).</p>
<pre class="r"><code>orig &lt;- demo_stim() # load demo images
mirrored &lt;- mirror(orig)
cropped  &lt;- crop(orig, width = 0.75, height = 0.75)
resized  &lt;- resize(orig, 0.75)
rotated  &lt;- rotate(orig, degrees = 180)
padded   &lt;- pad(orig, 30, fill = &quot;black&quot;)
grey     &lt;- greyscale(orig)</code></pre>

<div class="figure"><span style="display:block;" id="fig:editing"></span>
<img src="index_files/figure-html/editing-1.png" alt="Seven versions of the same average female face with different manipulations applied." width="100%" />
<p class="caption">
Figure 3: Examples of image manipulations: (A) original image, (B) mirrored, (C) cropped to 75%, (D) resized to 75%, (E) rotated 180 degrees, (F) 30 pixels of black padding added, and (G) greyscale.
</p>
</div>
</div>
<div id="delineation" class="section level2">
<h2>Delineation</h2>
<p>The image manipulations above work best if your raw images start the same size and aspect ratio, with the faces in the same orientation and position on each image. This is frequently not the case with raw images. Image delineation provides a way to set image manipulation parameters relative to face landmarks by marking corresponding points according to a template.</p>
<p>WebMorph.org’s default face template marks 189 points (Fig. <a href="#fig:delineate">4</a>). Some of these points have very clear anatomical locations, such as point 0 (“left pupil”), while others have only approximate placements and are used mainly for masking or preventing morphing artifacts from affecting the background of images, such as point 147 (“about 2cm to the left of the top of the left ear (creates oval around head)”). Template point numbering is 0-based because PsychoMorph was originally written in Java.</p>
<div class="figure"><span style="display:block;" id="fig:delineate"></span>
<img src="index_files/figure-html/delineate-1.png" alt="Average female face with the shape delineated using 189 white numeric labels and blue lines connecting the numbers." width="100%" />
<p class="caption">
Figure 4: Default webmorph FRL template
</p>
</div>
<p>The function <code>tem_def()</code> retrieves a template definition that includes point names, default coordinates, and the identity of the symmetrically matching point for mirroring or symmetrising images Table <a href="#tab:tem-def">2</a>.</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:tem-def">Table 2: </span>The first 10 landmark points of WebMorph.org’s default “FRL” template.
</caption>
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:left;">
name
</th>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
sym
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
left pupil
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
275
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
right pupil
</td>
<td style="text-align:right;">
284
</td>
<td style="text-align:right;">
275
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
top of left iris
</td>
<td style="text-align:right;">
165
</td>
<td style="text-align:right;">
267
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
top-left of left iris
</td>
<td style="text-align:right;">
156
</td>
<td style="text-align:right;">
270
</td>
<td style="text-align:right;">
17
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
left of left iris
</td>
<td style="text-align:right;">
154
</td>
<td style="text-align:right;">
277
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
bottom-left of left iris
</td>
<td style="text-align:right;">
157
</td>
<td style="text-align:right;">
283
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
bottom of left iris
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
286
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
bottom-right of left iris
</td>
<td style="text-align:right;">
174
</td>
<td style="text-align:right;">
283
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
right of left iris
</td>
<td style="text-align:right;">
177
</td>
<td style="text-align:right;">
276
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
top-right of left iris
</td>
<td style="text-align:right;">
175
</td>
<td style="text-align:right;">
270
</td>
<td style="text-align:right;">
11
</td>
</tr>
</tbody>
</table>
<p>You can automatically delineate faces with a simpler template (Fig. <a href="#fig:auto-delin">5</a>) using the online services provided through the free web platform Face++ <span class="citation">(2021)</span>, or dlib models provided by <a href="https://github.com/davisking/dlib-models">Davis King on a CC-0 license</a> and included in the <code>webmorphR.dlib</code> package.</p>
<pre class="r"><code># load 5 images with FRL templates
f &lt;- load_stim_neutral(&quot;006|038|064|066|135&quot;)

# remove templates and auto-delineate with dlib
# requires a python installation
dlib70_tem &lt;- auto_delin(f, &quot;dlib70&quot;, replace = TRUE)
dlib7_tem &lt;- auto_delin(f, &quot;dlib7&quot;, replace = TRUE)

# remove templates and auto-delineate with Face++
# requires a Face++ account; see ?webmorphR::auto_delin
fpp106_tem &lt;- auto_delin(f, &quot;fpp106&quot;, replace = TRUE)
fpp83_tem &lt;- auto_delin(f, &quot;fpp83&quot;, replace = TRUE)</code></pre>
<div class="figure"><span style="display:block;" id="fig:auto-delin"></span>
<img src="index_files/figure-html/auto-delin-1.png" alt="A West Asian female face showing dots and lines marking the full face (top row), three reduced template marking only the jaw from ear to ear and internal facial features, and one marking only the eyes and nose." width="100%" />
<p class="caption">
Figure 5: Delineation templates: (A) manual delineation using the FRL template, (B) automatic delineation using the Face++ 106-point template, (C) automatic delineation using the Face++ 83-point template, (D) automatic delineation using the 70-point dlib template, and (E) automatic delineation using the 7-point dlib template.
</p>
</div>
<p>A study comparing the accuracy of four common measures of face shape (sexual dimorphism, distinctiveness, bilateral asymmetry, and facial width to height ratio) between automatic and manual delineation concluded that automatic delineation had good correlations with manual delineation <span class="citation">(A. L. Jones et al., 2021)</span>. However, around 2% of images had noticeably inaccurate automatic delineation, which the authors emphasised should be screened for by outlier detection and visual inspection.</p>
<p>You can use the <code>delin()</code> function in webmorphR to open auto-delineated images in a visual editor to fix any inaccuracies.</p>
<pre class="r"><code>dlib7_tem_fixed &lt;- delin(dlib7_tem)</code></pre>
<div class="figure"><span style="display:block;" id="fig:delin-shiny"></span>
<img src="images/delin.png" alt="The shiny app interface for manual delineation adjustments." width="100%" />
<p class="caption">
Figure 6: The shiny app interface for manual delineation adjustments.
</p>
</div>
<p>While automatic delineation has the advantage of being very fast and generally more replicable than manual delineation, it is more limited in the areas that can be described. Typically, automatic face detection algorithms outline the lower face shape and internal features of the face, but don’t define the hairline, hair, neck, or ears. Manual delineation of these can greatly improve stimuli created through morphing or transforming (Fig. <a href="#fig:avg-comp">7</a>).</p>
<div class="figure"><span style="display:block;" id="fig:avg-comp"></span>
<img src="index_files/figure-html/avg-comp-1.png" alt="Averages of the 5 West Asian female faces. The right average has blurrier features then the left one, especially around the ears, neck and hairline." width="100%" />
<p class="caption">
Figure 7: Averages of 5 images made using (A) the full 189-point manual template and (B) the reduced 106-point automatic template.
</p>
</div>
</div>
<div id="facial-metrics" class="section level2">
<h2>Facial Metrics</h2>
<p>Once you have images delineated, you can use the x- and y-coordinates to calculate various facial-metric measurements (Table <a href="#tab:metrics">4</a>). Get all or a subset of points with the function <code>get_point()</code>. Remember, points are 0-based, so the first point (left pupil) is 0. This function returns a data table with one row for each point for each face.</p>
<pre class="r"><code>eye_points &lt;- get_point(f, pt = 0:1)</code></pre>
<table class="table table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-7">Table 3: </span>Coordinates of the first two points.
</caption>
<thead>
<tr>
<th style="text-align:left;">
image
</th>
<th style="text-align:right;">
point
</th>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
006_03
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
620
</td>
</tr>
<tr>
<td style="text-align:left;">
006_03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
776
</td>
<td style="text-align:right;">
630
</td>
</tr>
<tr>
<td style="text-align:left;">
038_03
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
580
</td>
<td style="text-align:right;">
580
</td>
</tr>
<tr>
<td style="text-align:left;">
038_03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
793
</td>
<td style="text-align:right;">
577
</td>
</tr>
<tr>
<td style="text-align:left;">
064_03
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
578
</td>
</tr>
<tr>
<td style="text-align:left;">
064_03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
783
</td>
<td style="text-align:right;">
570
</td>
</tr>
<tr>
<td style="text-align:left;">
066_03
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
562
</td>
<td style="text-align:right;">
595
</td>
</tr>
<tr>
<td style="text-align:left;">
066_03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
790
</td>
<td style="text-align:right;">
599
</td>
</tr>
<tr>
<td style="text-align:left;">
135_03
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
573
</td>
<td style="text-align:right;">
639
</td>
</tr>
<tr>
<td style="text-align:left;">
135_03
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
788
</td>
<td style="text-align:right;">
639
</td>
</tr>
</tbody>
</table>
<p>The <code>metrics()</code> function helps you quickly calculate the distance between any two points, such as the pupil centres, or use a more complicated formula, such as the face width-to-height ratio from Lefevre et al. <span class="citation">(2013)</span>.</p>
<pre class="r"><code># inter-pupillary distance between points 0 and 1
ipd &lt;- metrics(f, c(0, 1))

# face width-to-height ratio
left_cheek &lt;- metrics(f, &quot;min(x[110],x[111],x[109])&quot;)
right_cheek &lt;- metrics(f, &quot;max(x[113],x[112],x[114])&quot;)
bizygomatic_width &lt;- right_cheek - left_cheek
top_upper_lip &lt;- metrics(f, &quot;y[90]&quot;)
highest_eyelid &lt;- metrics(f, &quot;min(y[20],y[25])&quot;)
face_height &lt;- top_upper_lip - highest_eyelid
fwh &lt;- bizygomatic_width/face_height

# alternatively, do all calculations in one equation
fwh &lt;- metrics(f, &quot;abs(max(x[113],x[112],x[114])-min(x[110],x[111],x[109]))/abs(y[90]-min(y[20],y[25]))&quot;)</code></pre>
<table class="table table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:metrics">Table 4: </span>Facial metric measurements.
</caption>
<thead>
<tr>
<th style="text-align:left;">
face
</th>
<th style="text-align:right;">
x0
</th>
<th style="text-align:right;">
y0
</th>
<th style="text-align:right;">
x1
</th>
<th style="text-align:right;">
y1
</th>
<th style="text-align:right;">
ipd
</th>
<th style="text-align:right;">
fwh
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
006_03
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
620
</td>
<td style="text-align:right;">
776
</td>
<td style="text-align:right;">
630
</td>
<td style="text-align:right;">
206.2426
</td>
<td style="text-align:right;">
2.218905
</td>
</tr>
<tr>
<td style="text-align:left;">
038_03
</td>
<td style="text-align:right;">
580
</td>
<td style="text-align:right;">
580
</td>
<td style="text-align:right;">
793
</td>
<td style="text-align:right;">
577
</td>
<td style="text-align:right;">
213.0211
</td>
<td style="text-align:right;">
2.636580
</td>
</tr>
<tr>
<td style="text-align:left;">
064_03
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
578
</td>
<td style="text-align:right;">
783
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
213.1502
</td>
<td style="text-align:right;">
2.351220
</td>
</tr>
<tr>
<td style="text-align:left;">
066_03
</td>
<td style="text-align:right;">
562
</td>
<td style="text-align:right;">
595
</td>
<td style="text-align:right;">
790
</td>
<td style="text-align:right;">
599
</td>
<td style="text-align:right;">
228.0351
</td>
<td style="text-align:right;">
2.281818
</td>
</tr>
<tr>
<td style="text-align:left;">
135_03
</td>
<td style="text-align:right;">
573
</td>
<td style="text-align:right;">
639
</td>
<td style="text-align:right;">
788
</td>
<td style="text-align:right;">
639
</td>
<td style="text-align:right;">
215.0000
</td>
<td style="text-align:right;">
2.280788
</td>
</tr>
</tbody>
</table>
<p>While it is <em>possible</em> to calculate metrics such as width-to-height ratio from 2D face images, this does not mean it is a good idea. Even on highly standardized images, head tilt can have large effects on such measurements <span class="citation">(Hehman et al., 2013; Schneider et al., 2012)</span>. When image qualities such as camera type and head-to-camera distance are not standardized, facial metrics are meaningless at best <span class="citation">(Trebicky et al., 2016)</span>.</p>
</div>
<div id="alignment" class="section level2">
<h2>Alignment</h2>
<p>If your image set isn’t highly standardised, you probably want to crop, resize and rotate your images to get them all in approximately the same orientation on images of the same size. There are several reproducible options, each with pros and cons.</p>
<p>One-point alignment (Fig. <a href="#fig:norm-comp">8</a>A) doesn’t rotate or resize the image at all, but aligns one of the delineation points across images. This is ideal when you know that your camera-to-head distance and orientation was standard (or meaningfully different) across images and you want to preserve this in the stimuli, but you still need to get them all in the same position and image size.</p>
<p>Two-point alignment (Fig. <a href="#fig:norm-comp">8</a>B) resizes and rotates the images so that two points (usually the centres of the eyes) are in the same position on each image. This will alter relative head size such that people with very close-set eyes will appear to have larger heads than people with very wide-set eyes. This technique is good for getting images into the same orientation when you didn’t have any control over image rotation and camera-to-head distance of the original photos.</p>
<p>Procrustes alignment (Fig. <a href="#fig:norm-comp">8</a>C) resizes and rotates the images so that each delineation point is aligned as closely as possible across all images. This can obscure meaningful differences in relative face size (e.g., a baby’s face will be as large as an adult’s), but can be superior to two-point alignment. While this requires that the whole face be delineated, you can use a minimal template such as a face outline or the Face++ auto-delineation to achieve good results.</p>
<p>You can very quickly delineate an image set with a custom template using the <code>delin()</code> function in webmorphR if auto-delineation doesn’t provide suitable points.</p>
<pre class="r"><code># one-point alignment
onept &lt;- align(f, pt1 = 55, pt2 = 55,
               x1 = width(f)/2, y1 = height(f)/2,
               fill = &quot;dodgerblue&quot;)

# two-point alignment
twopt &lt;- align(f, pt1 = 0, pt2 = 1, fill = &quot;dodgerblue&quot;)

# procrustes alignment
proc &lt;- align(f, pt1 = 0, pt2 = 1, procrustes = TRUE, fill = &quot;dodgerblue&quot;)</code></pre>
<div class="figure"><span style="display:block;" id="fig:norm-comp"></span>
<img src="index_files/figure-html/norm-comp-1.png" alt="Five West Asian female faces on each row, with the alignments described in the caption." width="100%" />
<p class="caption">
Figure 8: Original images with different alignments. (A) One-point alignment placing the bottom of the nose point in the centre of the image. (B) Two-point alignment placing the eye centre points in the same position as the average image. (C) Procrustes alignment moved, rotated, and resized all images to most closely match the average face. A blue background was used to highlight the difference here, but normally a colour matching the image background would be used or the images would be cropped.
</p>
</div>
</div>
<div id="masking" class="section level2">
<h2>Masking</h2>
<p>Oftentimes, researchers will want to remove the background, hair, and clothing from an image. For example, the presence versus absence of hairstyle information can reverse preferences for masculine versus feminine male averages <span class="citation">(DeBruine et al., 2006)</span>.</p>
<p>The “standard oval mask” has enjoyed widespread popularity because it is straightforward to add to images using programs like PhotoShop, although the procedure usually requires some subjective judgements, as exemplified by this quote from <span class="citation">Hong Liu &amp; Chen (2018)</span>:</p>
<blockquote>
<p>The ‘oval’ mask, in contrast, was a predefined oval window that occluded a greater area of external features, including the jawline and the hairline. The ratio of oval width to oval height was 1:1.3. It was adjusted to fit for the size of the face.</p>
</blockquote>
<p>WebmorphR’s <code>mask_oval()</code> function allows you to set oval boundaries manually (Fig. <a href="#fig:mask">9</a>A) or in relation to minimum and maximum template coordinates for each face (Fig. <a href="#fig:mask">9</a>B) or across the full image set. An arguably better way to mask out hair, clothing and background from images is to crop around the curves defined by the template (Fig. <a href="#fig:mask">9</a>C).</p>
<pre class="r"><code># standard oval mask
bounds &lt;- list(t = 200, r = 400, b = 300, l = 400)
oval &lt;- mask_oval(f, bounds, fill = &quot;dodgerblue&quot;)

# template-aware oval mask
oval_tem &lt;- f |&gt;
  subset_tem(features(&quot;gmm&quot;)) |&gt; # remove external points
  mask_oval(fill = &quot;dodgerblue&quot;) # oval boundaries to max and min template points

# template-aware mask
masked &lt;- mask(f, c(&quot;face&quot;, &quot;neck&quot;, &quot;ears&quot;), fill = &quot;dodgerblue&quot;)</code></pre>
<div class="figure"><span style="display:block;" id="fig:mask"></span>
<img src="index_files/figure-html/mask-1.png" alt="Five West Asian female faces on each row, with the masking described in the caption." width="100%" />
<p class="caption">
Figure 9: Images masked with (A) an oval defined by image coordinates, (B) an oval defined by the minimum and maximum x- and y-coordinates of template points, or (C) to include face, ears and neck.
</p>
</div>
</div>
<div id="averaging" class="section level2">
<h2>Averaging</h2>
<p>Creating average images (also called composite or prototype images) through morphing can be a way to visualise the differences between groups of images <span class="citation">(Burton et al., 2005)</span>, manipulate averageness <span class="citation">(Little et al., 2011)</span>, or create prototypical faces for image transformations.</p>
<p>Averaging faces with texture <span class="citation">(Tiddeman et al., 2005, 2001)</span> makes composite images look more realistic (Fig. <a href="#fig:avg-texture">10</a>A). However, averages created without texture averaging look smoother and may be more appropriate for transforming color (Fig. <a href="#fig:avg-texture">10</a>B).</p>
<pre class="r"><code>avg_tex &lt;- avg(f, texture = TRUE)
avg_notex &lt;- avg(f, texture = FALSE)</code></pre>
<div class="figure"><span style="display:block;" id="fig:avg-texture"></span>
<img src="index_files/figure-html/avg-texture-1.png" alt="The average female West Asian face. The Top row shows the full face and the bottom row is a close-up of the eyes. The left image has cripsly defined features and skin texture, such as wrinkles and visible eyebrow hairs, while the right image has blurrier features, smoother skin, and individual hairs are not visible." width="100%" />
<p class="caption">
Figure 10: An average of 5 faces created (A) with texture averaging and (B) without.
</p>
</div>
</div>
<div id="transforming" class="section level2">
<h2>Transforming</h2>
<p>Transforming alters the appearance of one face by some proportion of the differences between two other faces. This technique is distinct from morphing. For example, you can transform a face in the dimension of sexual dimorphism by calculating the shape and color differences between a prototype female face (Fig. <a href="#fig:trans-vs-morph">11</a>A) and a prototype male face (Fig. <a href="#fig:trans-vs-morph">11</a>B). If you morph an individual female face with these images, you get faces that are halfway between the individual and prototype faces (Fig. <a href="#fig:trans-vs-morph">11</a>C,D). However, if you transform the individual face by 50% of the prototype differences, you get feminised and masculinized versions of the individual face (Fig. <a href="#fig:trans-vs-morph">11</a>E,F).</p>
<div class="figure"><span style="display:block;" id="fig:trans-vs-morph"></span>
<img src="index_files/figure-html/trans-vs-morph-1.png" alt="Six faces in three columns/two rows. The first column is average White faces, female on top and male on the bottom. The second column is the average of the faces in the first column and an individual White female face; the top image looks like a more average version of the individual face and the bottom image looks like an androgynous face. The third column is transforms of the individual female face; the top image looks more like a more feminine version of the individual face and the bottom image looks like a male version of the individual face." width="75%" />
<p class="caption">
Figure 11: Morphing versus transforming: (A) female and male composite images, (B) averages of the composites with the individual image, (C) transforms of the individual image along the male-female continuum.
</p>
</div>
<p>If, for example, the individual female face was more feminine than the average female face, morphing with the average female face produces an image that is <em>less</em> feminine than the original individual, while transforming along the male-female dimension produces and image that is always <em>more</em> feminine than the original. Morphing with a prototype also results in an image with increased averageness, while transforming maintains individually distinctive features.</p>
<p>Transforming also allows you to manipulate shape and colour independently (Fig. <a href="#fig:trans-shape-color">12</a>).</p>
<div class="figure"><span style="display:block;" id="fig:trans-shape-color"></span>
<img src="index_files/figure-html/trans-shape-color-1.png" alt="Transforming shape and color independently: (A) original individual image, (B) shape only, (C), color only, (D) both shape and color." width="100%" />
<p class="caption">
Figure 12: Transforming shape and color independently: (A) original individual image, (B) shape only, (C), color only, (D) both shape and color.
</p>
</div>
</div>
<div id="symmetrising" class="section level2">
<h2>Symmetrising</h2>
<p>Although a common technique <span class="citation">(e.g., Mealey et al., 1999)</span>, left-left and right-right mirroring (Fig. <a href="#fig:mirror-sym">13</a>) is not recommended for investigating perceptions of facial symmetry. As noted by <span class="citation">Perrett et al. (1999)</span>, this is because this method typically produces unnatural images for any face that isn’t already perfectly symmetric. For example, if the nose does not lie in a perfectly straight line from the centre point between the eyes to the centre of the mouth, then one of the mirrored halves will have a much wider nose than the original face, while the the other half will have a much narrower nose than the original face. In extreme cases, one mirrored version can end up with three nostrils and the other with a single nostril.</p>
<div class="figure"><span style="display:block;" id="fig:mirror-sym"></span>
<img src="index_files/figure-html/mirror-sym-1.png" alt="The five West Asian women shown left-left and right-right mirrored. Some look OK and others look bizarre, with unnaturally wider or narrow features." width="100%" />
<p class="caption">
Figure 13: Left-left (top) and right-right (bottom) mirrored images. The code for making these images is in the supplemental materials, but we only recommend using this method to demonstrate how misleading it is.
</p>
</div>
<p>A morph-based technique is a more realistic way to manipulate symmetry <span class="citation">(Little et al., 2001, 2011; Paukner et al., 2017; Perrett et al., 1999)</span>. It preserves the individual’s characteristic feature shapes and avoids the problem of having to choose an axis of symmetry on a face that isn’t perfectly symmetrical. In this method, the original face is mirror-reversed and each template point is re-labelled. The original and mirrored images are averaged together to create a perfectly symmetric version of the image that has the same feature widths as the original face (Fig. <a href="#fig:morph-sym">14</a>).</p>
<p>You can also use this symmetric version to create asymmetric versions of the original face through transforming: exaggerating the differences between the original and the symmetric version. This can be used, for example, to investigate perceptions of faces with exaggerated asymmetry <span class="citation">(Tybur et al., 2022)</span>, which has been hypothesised to be a cue of poor health during developmental.</p>
<pre class="r"><code>sym_both &lt;- symmetrize(f)
sym_shape &lt;- symmetrize(f, color = 0)
sym_color &lt;- symmetrize(f, shape = 0)
sym_anti &lt;- symmetrize(f, shape = -1.0, color = 0)</code></pre>
<div class="figure"><span style="display:block;" id="fig:morph-sym"></span>
<img src="index_files/figure-html/morph-sym-1.png" alt="Images with different types of symmetry: (A) symmetric shape and color, (B) symmetric color, (C) symmetric shape, (D) asymmetric shape." width="100%" />
<p class="caption">
Figure 14: Images with different types of symmetry: (A) symmetric shape and color, (B) symmetric color, (C) symmetric shape, (D) asymmetric shape.
</p>
</div>
</div>
</div>
<div id="case-studies" class="section level1">
<h1>Case Studies</h1>
<p>In this section, we will demonstrate how more complex face image manipulations can be scripted, such as the creation of prototype faces, making emotion continuua, manipulating sexual dimorphism, manipulating resemblance, and labelling stimuli with words or images.</p>
<div id="london-face-set" class="section level2">
<h2>London Face Set</h2>
<p>We will use the open-source, CC-BY licensed image set, the Face Research Lab London Set <span class="citation">(DeBruine &amp; Jones, 2017b)</span>. Images are of 102 adults whose pictures were taken in London, UK, in April 2012 for a project with Nikon camera (Fig. <a href="#fig:london-set">15</a>). All individuals were paid and gave signed consent for their images to be “used in lab-based and web-based studies in their original or altered forms and to illustrate research (e.g., in scientific journals, news media or presentations).”</p>
<div class="figure"><span style="display:block;" id="fig:london-set"></span>
<img src="index_files/figure-html/london-set-1.png" alt="The 102 neutral front faces in the London Face Set." width="100%" />
<p class="caption">
Figure 15: The 102 neutral front faces in the London Face Set.
</p>
</div>
<p>Each subject has one smiling and one neutral pose. For each pose, 5 full colour images were simultaneously taken from different angles: left profile, left three-quarter, front, right three-quarter, and right profile, but we will only use the front-facing images in the examples below. These images were cropped to 1350x1350 pixels and the faces were manually centered (many years ago before we made the tools in this paper). The neutral front images have template files that mark out 189 coordinates delineating face shape for use with Psychomorph or WebMorph.</p>
</div>
<div id="protoypes" class="section level2">
<h2>Protoypes</h2>
<p>The first step for many types of stimuli is to create prototype faces for some categories, such as expression or gender. The faces that make up these averages should be matched for other characteristics that you want to avoid confounding with the categories of interest, such as age or ethnicity. Here, we will choose 5 Black female faces, automatically delineate them, align the images, and create neutral and smiling prototypes (Fig. <a href="#fig:emo-avg">16</a>).</p>
<pre class="r"><code># select the relevant images and auto-delineate them
neu_orig &lt;- subset(london, face_gender == &quot;female&quot;) |&gt;
  subset(face_eth == &quot;black&quot;) |&gt; subset(1:5) |&gt;
  auto_delin(&quot;dlib70&quot;, replace = TRUE)

smi_orig &lt;- subset(smiling, face_gender == &quot;female&quot;) |&gt;
  subset(face_eth == &quot;black&quot;) |&gt; subset(1:5) |&gt;
  auto_delin(&quot;dlib70&quot;, replace = TRUE)

# align the images
all &lt;- c(neu_orig, smi_orig)
aligned &lt;- all |&gt;
  align(procrustes = TRUE, fill = patch(all)) |&gt;
  crop(.6, .8, y_off = 0.05)

neu &lt;- subset(aligned, 1:5)
smi &lt;- subset(aligned, 6:10)

neu_avg &lt;- avg(neu, texture = FALSE)
smi_avg &lt;- avg(smi, texture = FALSE)</code></pre>
<div class="figure"><span style="display:block;" id="fig:emo-avg"></span>
<img src="index_files/figure-html/emo-avg-1.png" alt="Average and individual neutral and smiling faces." width="100%" />
<p class="caption">
Figure 16: Average and individual neutral and smiling faces.
</p>
</div>
<p>We use the “dlib70” auto-delineation model, which is available through webmorphR.dlib <span class="citation">(DeBruine, 2022b)</span>, but requires the installation of python and some python packages. However, it has the advantage of not requiring setting up an account at Face++ and doesn’t transfer your images to a third party.</p>
</div>
<div id="emotion-continuum" class="section level2">
<h2>Emotion Continuum</h2>
<p>Once you have two prototype images, you can set up a continuum that morphs between the images and even exaggerates beyond them (Fig. <a href="#fig:continuum">17</a>). Note that some exaggerations beyond the prototypes can produce impossible shape configurations, such as the negative smile, where the open lips from a smile go to closed at 0% and pass through each other at negative values.</p>
<pre class="r"><code>steps &lt;- continuum(neu_avg, smi_avg, from = -0.5, to = 1.5, by = 0.25)</code></pre>

<div class="figure"><span style="display:block;" id="fig:continuum"></span>
<img src="index_files/figure-html/continuum-1.png" alt="Continuum from -50% to +150% smiling." width="100%" />
<p class="caption">
Figure 17: Continuum from -50% to +150% smiling.
</p>
</div>
</div>
<div id="sexual-dimorphism-transform" class="section level2">
<h2>Sexual dimorphism transform</h2>
<p>We can use the full templates to create sexual dimorphism transforms from neutral faces. Repeat the process above for 5 male and 5 female neutral faces, skipping the auto-delineation because these images already have webmorph templates (Fig. <a href="#fig:sexdim-avg">18</a>).</p>
<pre class="r"><code># select the relevant images
f_orig &lt;- subset(london, face_gender == &quot;female&quot;) |&gt;
  subset(face_eth == &quot;black&quot;) |&gt; subset(1:5)

m_orig &lt;- subset(london, face_gender == &quot;male&quot;) |&gt;
  subset(face_eth == &quot;black&quot;) |&gt; subset(1:5)

# align the images
all &lt;- c(f_orig, m_orig)
aligned &lt;- all |&gt;
  align(procrustes = TRUE, fill = patch(all)) |&gt;
  crop(.6, .8, y_off = 0.05)

f &lt;- subset(aligned, 1:5)
m &lt;- subset(aligned, 6:10)

f_avg &lt;- avg(f, texture = FALSE)
m_avg &lt;- avg(m, texture = FALSE)</code></pre>
<div class="figure"><span style="display:block;" id="fig:sexdim-avg"></span>
<img src="index_files/figure-html/sexdim-avg-1.png" alt="Average and individual female and male faces." width="100%" />
<p class="caption">
Figure 18: Average and individual female and male faces.
</p>
</div>
<p>Next, transform each individual image using the average female and male faces as transform endpoints (Fig. <a href="#fig:sexdim-transform">19</a>).</p>
<pre class="r"><code># use a named vector for shape to automatically rename the images
sexdim &lt;- trans(
  trans_img = c(f, m),
  from_img = f_avg,
  to_img = m_avg,
  shape = c(fem = -.5, masc = .5)
)</code></pre>

<div class="figure"><span style="display:block;" id="fig:sexdim-transform"></span>
<img src="index_files/figure-html/sexdim-transform-1.png" alt="Versions of individual faces with (A) 50% feminised shape and (B) 50% masculinized shape." width="100%" />
<p class="caption">
Figure 19: Versions of individual faces with (A) 50% feminised shape and (B) 50% masculinized shape.
</p>
</div>
</div>
<div id="self-resemblance-transform" class="section level2">
<h2>Self-resemblance transform</h2>
<p>Some research involves creating “virtual siblings” for participants to test how they perceive and behave towards strangers with phenotypic kinship cues <span class="citation">(DeBruine, 2004, 2005; DeBruine et al., 2011)</span>. As discussed in detail in DeBruine et al. <span class="citation">(2008)</span>, while morphing techniques are sufficient to create same-gender virtual siblings, transforming techniques are required to make other-gender virtual siblings without confounding self-resemblance with androgyny (Fig. <a href="#fig:virtual-sibs">20</a>).</p>
<pre class="r"><code>virtual_sis &lt;- trans(
  trans_img = f_avg,   # transform an average female face
  shape = 0.5,         # by 50% of the shape differences
  from_img = m_avg,    # between an average male face
  to_img = m) |&gt;       # and individual male faces
  mask(c(&quot;face&quot;, &quot;neck&quot;,&quot;ears&quot;))

virtual_bro &lt;- trans(
  trans_img = m_avg,   # transform an average male face
  shape = 0.5,         # by 50% of the shape differences
  from_img = m_avg,    # between an average male face
  to_img = m) |&gt;       # and individual male faces
  mask(c(&quot;face&quot;, &quot;neck&quot;,&quot;ears&quot;))</code></pre>
<div class="figure"><span style="display:block;" id="fig:virtual-sibs"></span>
<img src="index_files/figure-html/virtual-sibs-1.png" alt="Creating virtual siblings: (A) original images, (B) virtual brothers, (C) virtual sisters." width="100%" />
<p class="caption">
Figure 20: Creating virtual siblings: (A) original images, (B) virtual brothers, (C) virtual sisters.
</p>
</div>
</div>
<div id="labels" class="section level2">
<h2>Labels</h2>
<p>Many social perception studies require labelled images, such a minimal group designs. You can add custom labels and superimpose images on stimuli (Fig. <a href="#fig:label-comp">21</a>).</p>
<pre class="r"><code>flags &lt;- read_stim(&quot;images/flags&quot;)

ingroup &lt;- f |&gt;
  # pad 10% at the top with matching color
  pad(0.1, 0, 0, 0, fill = patch(f)) |&gt;
  label(&quot;Scottish&quot;, &quot;north&quot;, &quot;+0+10&quot;) |&gt;
  image_func(&quot;composite&quot;, flags$saltire$img,
              gravity = &quot;northeast&quot;, offset = &quot;+10+10&quot;)

outgroup &lt;- f |&gt;
  pad(0.1, 0, 0, 0, fill = patch(f)) |&gt;
  label(&quot;Welsh&quot;, &quot;north&quot;, &quot;+0+10&quot;) |&gt;
  image_func(&quot;composite&quot;, flags$ddraig$img,
             gravity = &quot;northeast&quot;, offset = &quot;+10+10&quot;)</code></pre>
<div class="figure"><span style="display:block;" id="fig:label-comp"></span>
<img src="index_files/figure-html/label-comp-1.png" alt="Stimuli with text labels and superimposed images." width="100%" />
<p class="caption">
Figure 21: Stimuli with text labels and superimposed images.
</p>
</div>
</div>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>Preparing your stimuli for face research in the ways described above has several benefits. Once the original scripts are written, you will be able to prepare new stimuli without manual intervention. It also makes the process of changing your mind about the experimental design much less painful. If you decide that the images actually should have been aligned prior to several steps, you only need to add a line of code and rerun your script, instead of start a whole manual process over from scratch. But even more important, providing reproducible scripts can allow others to build on your work with their own images. This is beneficial for generalisability, whether or not you can share your original images.</p>
<p>In this section, we will discuss a number of issues related to making sure research that uses face stimuli is ethical and methodologically robust. While these issues may not be directly related to stimulus reproducibility, they are important to discuss in a paper that aims to make it easier for people to do research with face images.</p>
<div id="ethical-issues" class="section level2">
<h2>Ethical Issues</h2>
<p>Research with identifiable faces has a number of ethical issues. This means it is not always possible to share the exact images used in a study. In this case, it is all the more important for the stimulus construction methods to be clear and reproducible. However, there are other ethical issues outside of image sharing that we feel are important to highlight in a paper discussing the use of face images in research.</p>
<p>The use of face photographs must respect participant consent and personal data privacy. Images that are “freely” available on the internet are a grey area and the ethical issues should be carefully considered by the researchers and relevant ethics board.</p>
<p>We strongly advise against using face images in research where there is a possibility of real-world consequences for the pictured individuals. For example, do not post identifiable images of real people on real dating sites without the explicit consent of the pictured individuals for that specific research.</p>
<p>The use of face image analysis should never be used to predict behaviour or as automatic screening. For example, face images cannot be used to predict criminality or decide who should proceed to the interview stage in a job application. This type of application is unethical because the training data is always biased. Face image analysis can be useful for researching what aspects of face images give rise to the <em>perception</em> of traits like trustworthiness, but should not be confused with the ability to detect <em>actual</em> behaviour. Researchers have a responsibility to consider how their research may be misused in this manner.</p>
</div>
<div id="natural-vs-standardised-source-images" class="section level2">
<h2>Natural vs standardised source images</h2>
<p>Most studies of face perception have used face images captured under standardised conditions (i.e., have used face images taken when factors such as depicted viewpoint, lighting conditions, and background are held constant). However, recently studies have begun to use more naturalistic, unstandardised images to explore the extent to which findings for perceptions of highly standardised images generalise to perceptions of more naturalistic images that better capture the wide range of viewing conditions in which we typically encounter faces <span class="citation">(Bainbridge et al., 2013; Jenkins et al., 2011)</span>. Although unsuitable for many research questions (e.g., those investigating the role of parameters measured from the images and underlying qualities of the individuals photographed), these ‘ambient images’ are well suited for investigating within-person variability in facial appearance or identifying the viewing conditions where perceivers use (or do not use) facial characteristics to form first impressions. Although WebmorphR can help process these ‘ambient images’, the delineations are mainly specialised for mostly front-facing faces. Profile face templates are available, however, and templates for any pose can be created.</p>
<pre class="r"><code># get default profile templates
left_profile &lt;- tem_def(33)
right_profile &lt;- tem_def(32)

# visualise templates
left_viz &lt;- viz_tem_def(left_profile)
right_viz &lt;- viz_tem_def(right_profile)</code></pre>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="index_files/figure-html/unnamed-chunk-24-1.png" alt="Left and right profile templates available via webmorph.org." width="100%" />
<p class="caption">
Figure 22: Left and right profile templates available via webmorph.org.
</p>
</div>
</div>
<div id="synthetic-faces" class="section level2">
<h2>Synthetic faces</h2>
<p>Recently Deep Learning methods have had a huge impact on machine learning and there has been a considerable amount of face related work undertaken. In particular, generative adversarial networks (GANs) are capable of generating random photo-realistic faces from an input vector sampled from a known distribution <span class="citation">(Gauthier, 2014; Goodfellow et al., 2014)</span>. Face-generating GANs are usually in the form of a convolutional neural network that takes the input vector in the form of a small pixel image with many channels, and through repeated convolutions and upsampling, or transpose convolutions, combined with pooling methods and non-linear activation functions, can generate a 3-channel RGB image. The generating networks are trained with the help of a second CNN, a discriminator network, that using convolutions, pooling /downsampling and non-linear activations to detect real vs fake images. Training is alternated between the generator network and the discriminator network, where the discriminator is trained to detect the fake images, then the generator is trained to fool the discriminator, and so on. GANs learn a face space, which can be further explored to enable alteration of attributes such as age, gender, or glasses in the generated images <span class="citation">(e.g., Y. Shen et al., 2020)</span>.</p>
<p>Cycle-GANs extend the use of GANs for what is known as image translation (what we refer to as transforms in this paper) such as altering age, sex, race <span class="citation">(J.-Y. Zhu et al., 2017)</span>. Cycle-GANs use an encoding-decoding network to transform an input image belonging to one class (e.g. male) into the corresponding image in the target class (e.g. female). Similar to GANs, cycle-GANs are trained with the use of discriminator networks, which are trained to detect fake outputs from the networks. In addition, cycle-GANs need to produce not just realistic images for the target class, but they need to be (in some sense) otherwise unchanged from the input image. To help ensure this is the case, the inverse transform is also learnt (e.g. from female to male), along with it’s own discriminator, and the training tries to ensure that the result of the transformation followed by the inverse transformation results in an image as close as possible to the original input.</p>
<p>These synthetic faces are perceived as real human face images under many circumstances <span class="citation">(B. Shen et al., 2021)</span>. The use of GANs and cycle-GANs has started to make its way into face perception research <span class="citation">(e.g. Dado et al., 2022; Zaltron et al., 2020)</span>, and its use will undoubtedly increase, but these methods need to be used with caution. Firstly, the trained networks are essentially “black boxes” controlled by millions of learnt parameters that are extremely difficult to interpret. A consequence and example of this is the vulnerability to adversarial attacks. For example, it is possible to find valid-looking input images that will fail catastrophically on the output images <span class="citation">(Kos et al., 2018)</span>. Secondly, the quantity of training data needed is prohibitive for some experiments, as is the computing power needed to learn the models, requiring the repeated training of 2 networks for GAN or 4 networks for cycle-GAN. The need for very large datasets means that that image datasets are typically scraped off the web, which can result in biases, and ethical issues around consent. Thirdly, training GANs and cycle-GANs is notoriously challenging, and without care they can suffer from mode collapse, non-convergence and instability <span class="citation">(Saxena &amp; Cao, 2021)</span>.</p>
</div>
<div id="judging-composites" class="section level2">
<h2>Judging composites</h2>
<p>In this section we will explain a serious caveat to research using composite faces that concludes something about group differences from judgements of a single pair or a small number of pairs of composites. Since we are making it easier to create composites, we do not want to inadvertently encourage research with this particular design.</p>
<p>As a concrete illustration, a recent paper by <span class="citation">Alper et al. (2021)</span> used faces from the Faceaurus database <span class="citation">(Holtzman, 2011b)</span>. “Holtzman (2011) standardized the assessment scores, computed average scores of self- and peer-reports, and ranked the face images based on the resulting scores. Then, prototypes for each of the personality dimensions were created by digitally combining 10 faces with the highest, and 10 faces with the lowest scores on the personality trait in question (Holtzman, 2011).” This was done separately for male and female faces.</p>
<p>With 105 observers, Holtzman found that the ability to detect the composite higher in a dark triad trait was greater than chance for all three traits for each sex. However, since scores on the three dark triad traits are positively correlated, the three pairs of composite faces are not independent. Indeed, Holtzman states that 5 individuals were in all three low composites for the male faces, while the overlap was less extreme in other cases. Alper and colleagues replicated these findings in three studies with Ns of 160, 318, and 402, the larger two of which were pre-registered.</p>
<p>While we commend both Holtzman and Alper, Bayrak, and Yilmaz for their transparency, data sharing, and material sharing, we argue that the original test has an effective N of 2, not 105, and that further replications using these images, such as those done by Alper, Bayrak, and Yilmaz, regardless of number of observers or preregistered status, lend no further weight of evidence to the assertion that dark triad traits are visible in physical appearance.</p>
<p>To explain this, we’ll use an analogy that has nothing to do with faces (bear with us). Imagine a researcher predicts that women born on odd days are taller than women born on even days. Ridiculous, right? So let’s simulate some data assuming that isn’t true. The code below samples 20 women from a population with a mean height of 158.1 cm and an SD of 5.7. Half are born on odd days and half on even days.</p>
<pre class="r"><code>set.seed(8675309)

stim_n &lt;- 10
height_m &lt;- 158.1
height_sd &lt;- 5.7

odd &lt;- rnorm(stim_n, height_m, height_sd)
even &lt;- rnorm(stim_n, height_m, height_sd)

t.test(odd, even)</code></pre>
<pre><code>##
##  Welch Two Sample t-test
##
## data:  odd and even
## t = 1.7942, df = 17.409, p-value = 0.09016
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.7673069  9.5977215
## sample estimates:
## mean of x mean of y
##  161.1587  156.7435</code></pre>
<p>A t-test shows no significant difference, which is unsurprising. We simulated the data from the same distribution, so we know for sure there is no real difference here. Now we’re going to average the height of the women with odd and even birthdays. So if we create a full-body composite of women born on odd days, she would be 161.2 cm tall, and a composite of women born on even days would be 156.7 cm tall.</p>
<p>If we ask 100 observers to look at these two composites, side-by-side, and judge which one looks taller, what do you imagine would happen? It’s likely that nearly all of them would judge the odd-birthday composite as taller. But let’s say that observers have to judge the composites independently, and they are pretty bad with height estimation, so their estimates for each composite have error with a standard deviation of 10 cm. We then compare their estimates for the odd-birthday composite with the estimate for the even-birthday composite in a paired-samples t-test.</p>
<pre class="r"><code>obs_n &lt;-100 # number of observers
error_sd &lt;- 10 # observer error

# add the error to the composite mean heights
odd_estimates &lt;- mean(odd) + rnorm(obs_n, 0, error_sd)
even_estimates &lt;- mean(even) + rnorm(obs_n, 0, error_sd)

t.test(odd_estimates, even_estimates, paired = TRUE)</code></pre>
<pre><code>##
##  Paired t-test
##
## data:  odd_estimates and even_estimates
## t = 3.3962, df = 99, p-value = 0.0009848
## alternative hypothesis: true mean difference is not equal to 0
## 95 percent confidence interval:
##  1.902821 7.250747
## sample estimates:
## mean difference
##        4.576784</code></pre>
<p>Now the women with odd birthdays are significantly taller than the women with even birthdays (p = 0.001). Or are they?</p>
<p>People tend to show high agreement on stereotypical social perceptions from the physical appearance of faces, even when physical appearance is not meaningfully associated with the traits being judged <span class="citation">(B. C. Jones et al., 2021; Todorov et al., 2008b; Zebrowitz &amp; Montepare, 2008)</span>. We can be sure that by chance alone, our two composites will be at least slightly different on any measure, even if they are drawn from identical populations. The smaller the number of stimuli that go into each composite, the larger the mean (unsigned) size of this difference. With only 10 stimuli per composite (like the Facesaurus composites), the mean unsigned effect size of the difference between composites from populations with no real difference is 0.35 (in units of SD of the original trait distribution). If our observers are accurate enough at perceiving this difference, or we run a very large number of observers, we are virtually guaranteed to find significant results every time. Additionally, there is a 50% chance that these results will be in the predicted direction, and this direction will be replicable across different samples of observers for the same image set.</p>
<p>So what does this mean for studies of the link between personality traits and facial appearance? The analogy with birth date and height holds. As long as there are facial morphologies that are even slightly consistently associated with the <em>perception</em> of a trait, then composites will not be identical in that morphology. Thus, even if that morphology is totally unassociated with the trait as measured by, e.g., personality scales or peer report (which is often the case), using the composite rating method will inflate the false positive rate for concluding a difference.</p>
<p>The smaller the number of stimuli that go into each composite, the greater the chance that they will be visibly different in morphology related to the judgement of interest, just by chance alone. The larger the number of observers or the better observers are at detecting small differences in this morphology, the more likely that “detection” will be significantly above chance. Repeating this with a new set of observers does not increase the amount of evidence you have for the association between the face morphology and the measured trait. You’ve only measured it once in one population of faces. If observers are your unit of analyses, you are making conclusions about whether the population of observers can detect the difference between your stimuli, you cannot generalise this to new stimulus sets.</p>
<p>So how should researchers test for differences in facial appearance between groups? Assessment of individual face images, combined with mixed effects models <span class="citation">(DeBruine &amp; Barr, 2021)</span>, can allow you to simultaneously account for variance in both observers and stimuli, avoiding the inflated false positives of the composite method (or aggregating ratings). People often use the composite method when they have too many images for any one observer to rate, but cross-classified mixed models can analyse data from counterbalanced trials or randomised subset allocation.</p>
<p>Another reason to use the composite rating method is when you are not ethically permitted to use individual faces in research, but are ethically permitted to use non-identifiable composite images. In this case, you can generate a large number of random composite pairs to construct the chance distribution. The equivalent to a p-value for this method is the proportion of the randomly paired composites that your target pair has a more extreme result than. While this method is too tedious to use when constructing composite faces manually, scripting allows you to automate such a task.</p>
<pre class="r"><code>set.seed(8675309) # for reproducibility

# load 20 faces
f &lt;- load_stim_canada(&quot;f&quot;) |&gt; resize(0.5)

# set to the number of random pairs you want
n_pairs &lt;- 5

# repeat this code n_pairs times
pairs &lt;- lapply(1:n_pairs, function (i) {
  # sample a random 10:10 split
  rand1 &lt;- sample(names(f), 10)
  rand2 &lt;- setdiff(names(f), rand1)

  # create composite images
  comp1 &lt;- avg(f[rand1])
  comp2 &lt;- avg(f[rand2])

  # save images with paired names
  nm1 &lt;- paste0(&quot;img_&quot;, i, &quot;_a&quot;)
  nm2 &lt;- paste0(&quot;img_&quot;, i, &quot;_b&quot;)
  write_stim(comp1, dir = &quot;images/composites&quot;, names = nm1)
  write_stim(comp2, dir = &quot;images/composites&quot;, names = nm2)
})</code></pre>
<div class="figure"><span style="display:block;" id="fig:rand-pair"></span>
<img src="index_files/figure-html/rand-pair-1.png" alt="Five random pairs of composites from a sample of 20 faces (10 in each composite). Can you spot any differences?" width="100%" />
<p class="caption">
Figure 23: Five random pairs of composites from a sample of 20 faces (10 in each composite). Can you spot any differences?
</p>
</div>
</div>
<div id="open-resources" class="section level2">
<h2>Open Resources</h2>
<p>In conclusion, we hope that this paper has convinced you that it is both possible and desirable to use scripting to prepare stimuli for face research. You can access more detailed tutorials for webmorph.org at <a href="https://debruine.github.io/webmorph/" class="uri">https://debruine.github.io/webmorph/</a> and for webmorphR at <a href="https://debruine.github.io/webmorphR/" class="uri">https://debruine.github.io/webmorphR/</a>. All image sets used in this tutorial are available on a CC-BY license at <a href="https://figshare.com/search?q=webmorph%20psychomorph">figshare</a> and all software is available open source. The code to reproduce this paper can be found at <a href="https://github.com/debruine/reprostim" class="uri">https://github.com/debruine/reprostim</a>.</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>We used R <span class="citation">(Version 4.2.0; R Core Team, 2022)</span> and the R-packages <em>dplyr</em> <span class="citation">(Version 1.0.10; Wickham et al., 2022)</span>, <em>kableExtra</em> <span class="citation">(Version 1.3.4; H. Zhu, 2021)</span>, <em>magick</em> <span class="citation">(Version 2.7.3; Ooms, 2021)</span>, <em>papaja</em> <span class="citation">(Version 0.1.1; Aust &amp; Barth, 2022)</span>, <em>webmorphR</em> <span class="citation">(Version 0.1.1.9001; DeBruine, 2022a, 2022b; DeBruine &amp; Jones, 2022)</span>, <em>webmorphR.dlib</em> <span class="citation">(Version 0.0.0.9003; DeBruine, 2022b)</span>, and <em>webmorphR.stim</em> <span class="citation">(Version 0.0.0.9002; DeBruine &amp; Jones, 2022)</span> to produce this manuscript.</p>
<div id="refs" class="references csl-bib-body hanging-indent" custom-style="Bibliography" line-spacing="2">
<div id="ref-alper2021all" class="csl-entry">
Alper, S., Bayrak, F., &amp; Yilmaz, O. (2021). All the dark triad and some of the big five traits are visible in the face. <em>Personality and Individual Differences</em>, <em>168</em>, 110350. https://doi.org/<a href="https://doi.org/10.1016/j.paid.2020.110350">https://doi.org/10.1016/j.paid.2020.110350</a>
</div>
<div id="ref-R-papaja" class="csl-entry">
Aust, F., &amp; Barth, M. (2022). <em><span class="nocase">papaja</span>: <span>Prepare</span> reproducible <span>APA</span> journal articles with <span>R Markdown</span></em>. <a href="https://github.com/crsh/papaja">https://github.com/crsh/papaja</a>
</div>
<div id="ref-bainbridge2013intrinsic" class="csl-entry">
Bainbridge, W. A., Isola, P., &amp; Oliva, A. (2013). The intrinsic memorability of face photographs. <em>Journal of Experimental Psychology: General</em>, <em>142</em>(4), 1323.
</div>
<div id="ref-barrgeneralizing" class="csl-entry">
Barr, D. J. (2007). Generalizing over encounters. In <em>The oxford handbook of psycholinguistics</em>. Oxford University Press, USA.
</div>
<div id="ref-benson1991perception" class="csl-entry">
Benson, P. J., &amp; Perrett, D. I. (1991a). Perception and recognition of photographic quality facial caricatures: Implications for the recognition of natural images. <em>European Journal of Cognitive Psychology</em>, <em>3</em>(1), 105–135.
</div>
<div id="ref-benson1991synthesising" class="csl-entry">
Benson, P. J., &amp; Perrett, D. I. (1991b). Synthesising continuous-tone caricatures. <em>Image and Vision Computing</em>, <em>9</em>(2), 123–129.
</div>
<div id="ref-benson1993extracting" class="csl-entry">
Benson, P. J., &amp; Perrett, D. I. (1993). Extracting prototypical facial images from exemplars. <em>Perception</em>, <em>22</em>(3), 257–262.
</div>
<div id="ref-burton2005robust" class="csl-entry">
Burton, A. M., Jenkins, R., Hancock, P. J., &amp; White, D. (2005). Robust representations for face recognition: The power of averages. <em>Cognitive Psychology</em>, <em>51</em>(3), 256–284.
</div>
<div id="ref-dado2022hyperrealistic" class="csl-entry">
Dado, T., Güçlütürk, Y., Ambrogioni, L., Ras, G., Bosch, S., Gerven, M. van, &amp; Güçlü, U. (2022). Hyperrealistic neural decoding for reconstructing faces from fMRI activations via the GAN latent space. <em>Scientific Reports</em>, <em>12</em>(1), 1–9.
</div>
<div id="ref-webmorph" class="csl-entry">
DeBruine, L. M. (2018). <em>Webmorph: Beta release 2</em> (Version v0.0.0.9001) [Computer software]. Zenodo. <a href="https://doi.org/10.5281/zenodo.1162670">https://doi.org/10.5281/zenodo.1162670</a>
</div>
<div id="ref-DeBruine_2004PRSLB" class="csl-entry">
DeBruine, L. M. (2004). Facial resemblance increases the attractiveness of same-sex faces more than other-sex faces. <em>Proceedings of the Royal Society of London B</em>, <em>271</em>, 2085–2090. <a href="https://doi.org/10.1098/rspb.2004.2824">https://doi.org/10.1098/rspb.2004.2824</a>
</div>
<div id="ref-DeBruine_2005PRSLB" class="csl-entry">
DeBruine, L. M. (2005). Trustworthy but not lust-worthy: Context-specific effects of facial resemblance. <em>Proceedings of the Royal Society of London B</em>, <em>272</em>, 919–922. <a href="https://doi.org/10.1098/rspb.2004.3003">https://doi.org/10.1098/rspb.2004.3003</a>
</div>
<div id="ref-stim-composites" class="csl-entry">
DeBruine, L. M. (2016). <em>Young adult composite faces</em>. figshare. <a href="https://doi.org/10.6084/m9.figshare.4055130.v1">https://doi.org/10.6084/m9.figshare.4055130.v1</a>
</div>
<div id="ref-R-webmorphR" class="csl-entry">
DeBruine, L. M. (2022a). <em><span class="nocase">webmorphR</span>: Reproducible stimuli</em>. Zenodo. <a href="https://doi.org/10.5281/zenodo.6570965">https://doi.org/10.5281/zenodo.6570965</a>
</div>
<div id="ref-R-webmorphR.dlib" class="csl-entry">
DeBruine, L. M. (2022b). <em><span class="nocase">webmorphR.dlib</span>: Face detection for webmorphR</em>. <a href="https://debruine.github.io/webmorphR.dlib/">https://debruine.github.io/webmorphR.dlib/</a>
</div>
<div id="ref-debruine2021understanding" class="csl-entry">
DeBruine, L. M., &amp; Barr, D. J. (2021). Understanding mixed-effects models through data simulation. <em>Advances in Methods and Practices in Psychological Science</em>, <em>4</em>(1), 2515245920965119.
</div>
<div id="ref-Canada2003" class="csl-entry">
DeBruine, L. M., &amp; Jones, B. C. (2017a). <em>Young adult white faces with manipulated versions</em>. figshare. <a href="https://doi.org/10.6084/m9.figshare.4220517.v1">https://doi.org/10.6084/m9.figshare.4220517.v1</a>
</div>
<div id="ref-FRL_London" class="csl-entry">
DeBruine, L. M., &amp; Jones, B. C. (2017b). <em>Face research lab london set</em>. figshare. <a href="https://doi.org/10.6084/m9.figshare.5047666.v5">https://doi.org/10.6084/m9.figshare.5047666.v5</a>
</div>
<div id="ref-stim-3dsk" class="csl-entry">
DeBruine, L. M., &amp; Jones, B. C. (2020). <em>3DSK face set with webmorph templates</em>. Open Science Framework. <a href="https://doi.org/10.17605/OSF.IO/A3947">https://doi.org/10.17605/OSF.IO/A3947</a>
</div>
<div id="ref-R-webmorphR.stim" class="csl-entry">
DeBruine, L. M., &amp; Jones, B. C. (2022). <em><span class="nocase">webmorphR.stim</span>: Stimulus sets for webmorphR</em>. <a href="https://debruine.github.io/webmorphR.stim/">https://debruine.github.io/webmorphR.stim/</a>
</div>
<div id="ref-debruine2006correlated" class="csl-entry">
DeBruine, L. M., Jones, B. C., Little, A. C., Boothroyd, L. G., Perrett, D. I., Penton-Voak, I. S., Cooper, P. A., Penke, L., Feinberg, D. R., &amp; Tiddeman, B. P. (2006). Correlated preferences for facial masculinity and ideal or actual partner’s masculinity. <em>Proceedings of the Royal Society B: Biological Sciences</em>, <em>273</em>(1592), 1355–1360.
</div>
<div id="ref-DeBruine_2008ASB" class="csl-entry">
DeBruine, L. M., Jones, B. C., Little, A. C., &amp; Perrett, D. I. (2008). Social perception of facial resemblance in humans. <em>Archives of Sexual Behavior</em>, <em>37</em>, 64–77. <a href="https://doi.org/10.1007/s10508-007-9266-0">https://doi.org/10.1007/s10508-007-9266-0</a>
</div>
<div id="ref-DeBruine2007JEPHPP" class="csl-entry">
DeBruine, L. M., Jones, B. C., Unger, L., Little, A. C., &amp; Feinberg, D. R. (2007). Dissociating averageness and attractiveness: Attractive faces are not always average. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>33</em>, 1420–1430. <a href="https://doi.org/10.1037/0096-1523.33.6.1420">https://doi.org/10.1037/0096-1523.33.6.1420</a>
</div>
<div id="ref-DeBruine_2011PNAS" class="csl-entry">
DeBruine, L. M., Jones, B. C., Watkins, C. D., Roberts, S. C., Little, A. C., Smith, F. G., &amp; Quist, M. (2011). Opposite-sex siblings decrease attraction, but not prosocial attributions, to self-resembling opposite-sex faces. <em>Proceedings of the National Academy of Sciences</em>, <em>108</em>, 11710–11714. <a href="https://doi.org/10.1073/pnas.1105919108">https://doi.org/10.1073/pnas.1105919108</a>
</div>
<div id="ref-duchaine2015revised" class="csl-entry">
Duchaine, B., &amp; Yovel, G. (2015). A revised neural framework for face processing. <em>Annual Review of Vision Science</em>, <em>1</em>, 393–416.
</div>
<div id="ref-ekman1976pictures" class="csl-entry">
Ekman, P. (1976). Pictures of facial affect. <em>Consulting Psychologists Press</em>.
</div>
<div id="ref-faceplusplus" class="csl-entry">
Face++. (2021). Face++ AI open platform. In <em>Face++</em>. <a href="https://www.faceplusplus.com/landmarks/">https://www.faceplusplus.com/landmarks/</a>
</div>
<div id="ref-gauthier2014conditional" class="csl-entry">
Gauthier, J. (2014). Conditional generative adversarial nets for convolutional face generation. <em>Class Project for Stanford Cs231n: Convolutional Neural Networks for Visual Recognition, Winter Semester</em>, <em>2014</em>(5), 2.
</div>
<div id="ref-gonzalez2002digital" class="csl-entry">
Gonzalez, R. C., Woods, R. E., et al. (2002). <em>Digital image processing</em>. Prentice Hall Upper Saddle River, NJ. <a href="https://www.pearson.com/us/higher-education/product/Gonzalez-Digital-Image-Processing-2nd-Edition/9780201180756.html">https://www.pearson.com/us/higher-education/product/Gonzalez-Digital-Image-Processing-2nd-Edition/9780201180756.html</a>
</div>
<div id="ref-goodfellow2014generative" class="csl-entry">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). <em>Generative adversarial nets in: Advances in neural information processing systems (NIPS)</em>. Springer New York.
</div>
<div id="ref-Gronenschild_2009" class="csl-entry">
Gronenschild, E. H. B. M., Smeets, F., Vuurman, E. F. P. M., Boxtel, M. P. J. van, &amp; Jolles, J. (2009). The use of faces as stimuli in neuroimaging and psychological experiments: A procedure to standardize stimulus features. <em>Behavior Research Methods</em>, <em>41</em>, 1053–1060. <a href="https://doi.org/10.3758/BRM.41.4.1053">https://doi.org/10.3758/BRM.41.4.1053</a>
</div>
<div id="ref-hehman2013enhancing" class="csl-entry">
Hehman, E., Leitner, J. B., &amp; Gaertner, S. L. (2013). Enhancing static facial features increases intimidation. <em>Journal of Experimental Social Psychology</em>, <em>49</em>(4), 747–754. <a href="https://doi.org/10.1016/j.jesp.2013.02.015">https://doi.org/10.1016/j.jesp.2013.02.015</a>
</div>
<div id="ref-higham2016matlab" class="csl-entry">
Higham, D. J., &amp; Higham, N. J. (2016). <em>MATLAB guide</em> (Vol. 150). Siam.
</div>
<div id="ref-Holtzman_2011" class="csl-entry">
Holtzman, N. S. (2011a). Facing a psychopath: Detecting the dark triad from emotionally-neutral faces, using prototypes from the personality faceaurus. <em>Journal of Research in Personality</em>, <em>45</em>(6), 648–654.
</div>
<div id="ref-holtzman2011facing" class="csl-entry">
Holtzman, N. S. (2011b). Facing a psychopath: Detecting the dark triad from emotionally-neutral faces, using prototypes from the personality faceaurus. <em>Journal of Research in Personality</em>, <em>45</em>(6), 648–654.
</div>
<div id="ref-holzleitner2019comparing" class="csl-entry">
Holzleitner, I. J., Lee, A. J., Hahn, A. C., Kandrik, M., Bovet, J., Renoult, J. P., Simmons, D., Garrod, O., DeBruine, L. M., &amp; Jones, B. C. (2019). Comparing theory-driven and data-driven attractiveness models using images of real women’s faces. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>45</em>(12), 1589.
</div>
<div id="ref-hong2018boundary" class="csl-entry">
Hong Liu, C., &amp; Chen, W. (2018). The boundary of holistic processing in the appraisal of facial attractiveness. <em>Royal Society Open Science</em>, <em>5</em>(6), 171616.
</div>
<div id="ref-jack2017toward" class="csl-entry">
Jack, R. E., &amp; Schyns, P. G. (2017). Toward a social psychophysics of face communication. <em>Annual Review of Psychology</em>, <em>68</em>, 269–297.
</div>
<div id="ref-jenkins2011variability" class="csl-entry">
Jenkins, R., White, D., Van Montfort, X., &amp; Burton, A. M. (2011). Variability in photos of the same face. <em>Cognition</em>, <em>121</em>(3), 313–323.
</div>
<div id="ref-jones2021facial" class="csl-entry">
Jones, A. L., Schild, C., &amp; Jones, B. C. (2021). Facial metrics generated from manually and automatically placed image landmarks are highly correlated. <em>Evolution and Human Behavior</em>, <em>42</em>(3), 186–193. <a href="https://doi.org/10.1016/j.evolhumbehav.2020.09.002">https://doi.org/10.1016/j.evolhumbehav.2020.09.002</a>
</div>
<div id="ref-jones2021world" class="csl-entry">
Jones, B. C., DeBruine, L. M., Flake, J. K., Liuzza, M. T., Antfolk, J., Arinze, N. C., Ndukaihe, I. L. G., Bloxsom, N. G., Lewis, S. C., Foroni, F., et al. (2021). To which world regions does the valence–dominance model of social perception apply? <em>Nature Human Behaviour</em>, <em>5</em>(1), 159–169.
</div>
<div id="ref-jones_nomasc_2018" class="csl-entry">
Jones, B. C., Hahn, A. C., Fisher, C. I., Wang, H., Kandrik, M., Han, C., Fasolt, V., Morrison, D., Lee, A. J., Holzleitner, I. J., O’Shea, K. J., Roberts, S. C., Little, A. C., &amp; DeBruine, L. M. (2018). No <span>Compelling</span> <span>Evidence</span> that <span>Preferences</span> for <span>Facial</span> <span>Masculinity</span> <span>Track</span> <span>Changes</span> in <span>Women</span>’s <span>Hormonal</span> <span>Status</span>. <em>Psychological Science</em>, <em>29</em>(6), 996–1005. <a href="https://doi.org/10.1177/0956797618760197">https://doi.org/10.1177/0956797618760197</a>
</div>
<div id="ref-kos2018adversarial" class="csl-entry">
Kos, J., Fischer, I., &amp; Song, D. (2018). Adversarial examples for generative models. <em>2018 Ieee Security and Privacy Workshops (Spw)</em>, 36–42.
</div>
<div id="ref-lefevre2013telling" class="csl-entry">
Lefevre, C. E., Lewis, G. J., Perrett, D. I., &amp; Penke, L. (2013). Telling facial metrics: Facial width is associated with testosterone levels in men. <em>Evolution and Human Behavior</em>, <em>34</em>(4), 273–279.
</div>
<div id="ref-little2001self" class="csl-entry">
Little, A. C., Burt, D. M., Penton-Voak, I. S., &amp; Perrett, D. I. (2001). Self-perceived attractiveness influences human female preferences for sexual dimorphism and symmetry in male faces. <em>Proceedings of the Royal Society of London. Series B: Biological Sciences</em>, <em>268</em>(1462), 39–44.
</div>
<div id="ref-Little_2011" class="csl-entry">
Little, A. C., Jones, B. C., &amp; DeBruine, L. M. (2011). Facial attractiveness: Evolutionary based research. <em>Philosophical Transactions of the Royal Society B</em>, <em>366</em>, 1638–1659. <a href="https://doi.org/10.1098/rstb.2010.0404">https://doi.org/10.1098/rstb.2010.0404</a>
</div>
<div id="ref-CFD_2015" class="csl-entry">
Ma, D. S., Correll, J., &amp; Wittenbrink, B. (2015). The <span>Chicago</span> face database: A free stimulus set of faces and norming data. <em>Behavior Research Methods</em>, <em>47</em>, 1122–1135. <a href="https://doi.org/10.3758/s13428-014-0532-5">https://doi.org/10.3758/s13428-014-0532-5</a>
</div>
<div id="ref-Mealey_1999" class="csl-entry">
Mealey, L., Bridgstock, R., &amp; Townsend, G. C. (1999). Symmetry and perceived facial attractiveness: A monozygotic co-twin comparison. <em>Journal of Personality and Social Psychology</em>, <em>76</em>(1), 151.
</div>
<div id="ref-Morrison_2018" class="csl-entry">
Morrison, D., Wang, H., Hahn, A. C., Jones, B. C., &amp; DeBruine, L. M. (2018). <em>Predicting the reward value of faces and bodies from social perceptions: Supplemental materials</em>. OSF. <a href="https://doi.org/10.17605/OSF.IO/G27WF">https://doi.org/10.17605/OSF.IO/G27WF</a>
</div>
<div id="ref-nishimura2000graphicconverter" class="csl-entry">
Nishimura, D. (2000). GraphicConverter 3.9. 1. <em>Biotech Software &amp; Internet Report: The Computer Software Journal for Scient</em>, <em>1</em>(6), 267–269.
</div>
<div id="ref-o2011adaptation" class="csl-entry">
O’Neil, S. F., &amp; Webster, M. A. (2011). Adaptation and the perception of facial age. <em>Visual Cognition</em>, <em>19</em>(4), 534–550.
</div>
<div id="ref-olivola_social_2014" class="csl-entry">
Olivola, C. Y., Funk, F., &amp; Todorov, A. (2014). Social attributions from faces bias human choices. <em>Trends in Cognitive Sciences</em>, <em>18</em>(11), 566–570. <a href="https://doi.org/10.1016/j.tics.2014.09.007">https://doi.org/10.1016/j.tics.2014.09.007</a>
</div>
<div id="ref-R-magick" class="csl-entry">
Ooms, J. (2021). <em>Magick: Advanced graphics and image-processing in r</em>. <a href="https://CRAN.R-project.org/package=magick">https://CRAN.R-project.org/package=magick</a>
</div>
<div id="ref-paluszek2019pattern" class="csl-entry">
Paluszek, M., &amp; Thomas, S. (2019). Pattern recognition with deep learning. In <em>MATLAB machine learning recipes</em> (pp. 209–230). Springer.
</div>
<div id="ref-paukner2017capuchin" class="csl-entry">
Paukner, A., Wooddell, L. J., Lefevre, C. E., Lonsdorf, E., &amp; Lonsdorf, E. (2017). Do capuchin monkeys (sapajus apella) prefer symmetrical face shapes? <em>Journal of Comparative Psychology</em>, <em>131</em>(1), 73.
</div>
<div id="ref-pegors2015simultaneous" class="csl-entry">
Pegors, T. K., Mattar, M. G., Bryan, P. B., &amp; Epstein, R. A. (2015). Simultaneous perceptual and response biases on sequential face attractiveness judgments. <em>Journal of Experimental Psychology: General</em>, <em>144</em>(3), 664.
</div>
<div id="ref-perrett1999symmetry" class="csl-entry">
Perrett, D. I., Burt, D. M., Penton-Voak, I. S., Lee, K. J., Rowland, D. A., &amp; Edwards, R. (1999). Symmetry and human facial attractiveness. <em>Evolution and Human Behavior</em>, <em>20</em>(5), 295–307.
</div>
<div id="ref-perrett1998effects" class="csl-entry">
Perrett, D. I., Lee, K. J., Penton-Voak, I., Rowland, D., Yoshikawa, S., Burt, D. M., Henzi, S., Castles, D. L., &amp; Akamatsu, S. (1998). Effects of sexual dimorphism on facial attractiveness. <em>Nature</em>, <em>394</em>(6696), 884–887.
</div>
<div id="ref-perrett1994facial" class="csl-entry">
Perrett, D. I., May, K. A., &amp; Yoshikawa, S. (1994). Facial shape and judgements of female attractiveness. <em>Nature</em>, <em>368</em>(6468), 239–242.
</div>
<div id="ref-R-base" class="csl-entry">
R Core Team. (2022). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>
</div>
<div id="ref-rhodes2017adaptive" class="csl-entry">
Rhodes, G. (2017). Adaptive coding and face recognition. <em>Current Directions in Psychological Science</em>, <em>26</em>(3), 218–224.
</div>
<div id="ref-rhodes2011adaptive" class="csl-entry">
Rhodes, G., &amp; Leopold, D. A. (2011). Adaptive norm-based coding of face identity. <em>The Oxford Handbook of Face Perception</em>, 263–286.
</div>
<div id="ref-rhodes2001attractiveness" class="csl-entry">
Rhodes, G., Yoshikawa, S., Clark, A., Lee, K., McKay, R., &amp; Akamatsu, S. (2001). Attractiveness of facial averageness and symmetry in non-western cultures: In search of biologically based standards of beauty. <em>Perception</em>, <em>30</em>(5), 611–625. <a href="https://doi.org/10.1068/p3123">https://doi.org/10.1068/p3123</a>
</div>
<div id="ref-rowland1995manipulating" class="csl-entry">
Rowland, D. A., &amp; Perrett, D. I. (1995). Manipulating facial appearance through shape and color. <em>IEEE Computer Graphics and Applications</em>, <em>15</em>(5), 70–76.
</div>
<div id="ref-saxena2021generative" class="csl-entry">
Saxena, D., &amp; Cao, J. (2021). Generative adversarial networks (GANs) challenges, solutions, and future directions. <em>ACM Computing Surveys (CSUR)</em>, <em>54</em>(3), 1–42.
</div>
<div id="ref-Scheib_1999" class="csl-entry">
Scheib, J. E., Gangestad, S. W., &amp; Thornhill, R. (1999). Facial attractiveness, symmetry and cues of good genes. <em>Proceedings of the Royal Society of London. Series B: Biological Sciences</em>, <em>266</em>(1431), 1913–1917.
</div>
<div id="ref-schneider2012judging" class="csl-entry">
Schneider, T. M., Hecht, H., &amp; Carbon, C.-C. (2012). Judging body weight from faces: The height—weight illusion. <em>Perception</em>, <em>41</em>(1), 121–124.
</div>
<div id="ref-sforza2010my" class="csl-entry">
Sforza, A., Bufalari, I., Haggard, P., &amp; Aglioti, S. M. (2010). My face in yours: Visuo-tactile facial stimulation influences sense of identity. <em>Social Neuroscience</em>, <em>5</em>(2), 148–162.
</div>
<div id="ref-shen2021study" class="csl-entry">
Shen, B., RichardWebster, B., O’Toole, A., Bowyer, K., &amp; Scheirer, W. J. (2021). A study of the human perception of synthetic faces. <em>2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</em>, 1–8.
</div>
<div id="ref-shen2020interfacegan" class="csl-entry">
Shen, Y., Yang, C., Tang, X., &amp; Zhou, B. (2020). Interfacegan: Interpreting the disentangled face representation learned by gans. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.
</div>
<div id="ref-stephen2012cross" class="csl-entry">
Stephen, I. D., Scott, I. M., Coetzee, V., Pound, N., Perrett, D. I., &amp; Penton-Voak, I. S. (2012). Cross-cultural effects of color, but not morphological masculinity, on perceived attractiveness of men’s faces. <em>Evolution and Human Behavior</em>, <em>33</em>(4), 260–267.
</div>
<div id="ref-imagemagick" class="csl-entry">
The ImageMagick Development Team. (2021). <em>ImageMagick</em> (Version 7.0.10) [Computer software]. <a href="https://imagemagick.org">https://imagemagick.org</a>
</div>
<div id="ref-tiddeman2001prototyping" class="csl-entry">
Tiddeman, B. P., Burt, D. M., &amp; Perrett, D. I. (2001). Prototyping and transforming facial textures for perception research. <em>IEEE Computer Graphics and Applications</em>, <em>21</em>(5), 42–50.
</div>
<div id="ref-tiddeman2005towards" class="csl-entry">
Tiddeman, B. P., Stirrat, M. R., &amp; Perrett, D. I. (2005). Towards realism in facial image transformation: Results of a wavelet MRF method. <em>Computer Graphics Forum</em>, <em>24</em>, 449–456.
</div>
<div id="ref-todorov_understanding_2008" class="csl-entry">
Todorov, A., Said, C. P., Engell, A. D., &amp; Oosterhof, N. N. (2008a). Understanding evaluation of faces on social dimensions. <em>Trends in Cognitive Sciences</em>, <em>12</em>(12), 455–460. <a href="https://doi.org/10.1016/j.tics.2008.10.001">https://doi.org/10.1016/j.tics.2008.10.001</a>
</div>
<div id="ref-todorov2008understanding" class="csl-entry">
Todorov, A., Said, C. P., Engell, A. D., &amp; Oosterhof, N. N. (2008b). Understanding evaluation of faces on social dimensions. <em>Trends in Cognitive Sciences</em>, <em>12</em>(12), 455–460.
</div>
<div id="ref-tvrebicky2016focal" class="csl-entry">
Trebicky, V., Fialova, J., Kleisner, K., &amp; Havlicek, J. (2016). Focal length affects depicted shape and perception of facial images. <em>PLoS One</em>, <em>11</em>(2), e0149313.
</div>
<div id="ref-tybur2022re" class="csl-entry">
Tybur, J. M., Fan, L., Jones, B. C., Holzleitner, I. J., Lee, A. J., &amp; DeBruine, L. M. (2022). Re-evaluating the relationship between pathogen avoidance and preferences for facial symmetry and sexual dimorphism: A registered report. <em>Evolution and Human Behavior</em>, <em>43</em>(3), 212–223.
</div>
<div id="ref-visconti2014facilitated" class="csl-entry">
Visconti di Oleggio Castello, M., Guntupalli, J. S., Yang, H., &amp; Gobbini, M. I. (2014). Facilitated detection of social cues conveyed by familiar faces. <em>Frontiers in Human Neuroscience</em>, <em>8</em>, 678.
</div>
<div id="ref-wang2019detecting" class="csl-entry">
Wang, S.-Y., Wang, O., Owens, A., Zhang, R., &amp; Efros, A. A. (2019). Detecting photoshopped faces by scripting photoshop. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 10072–10081.
</div>
<div id="ref-webster2011visual" class="csl-entry">
Webster, M. A., &amp; MacLeod, D. I. (2011). Visual adaptation and face perception. <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em>, <em>366</em>(1571), 1702–1725.
</div>
<div id="ref-Weigelt_2013" class="csl-entry">
Weigelt, S., Koldewyn, K., &amp; Kanwisher, N. (2013). Face recognition deficits in autism spectrum disorders are both domain specific and process specific. <em>PloS One</em>, <em>8</em>(9), e74541.
</div>
<div id="ref-R-dplyr" class="csl-entry">
Wickham, H., François, R., Henry, L., &amp; Müller, K. (2022). <em>Dplyr: A grammar of data manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr">https://CRAN.R-project.org/package=dplyr</a>
</div>
<div id="ref-zaltron2020cg" class="csl-entry">
Zaltron, N., Zurlo, L., &amp; Risi, S. (2020). Cg-gan: An interactive evolutionary gan-based approach for facial composite generation. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>34</em>, 2544–2551.
</div>
<div id="ref-zebrowitz2008social" class="csl-entry">
Zebrowitz, L. A., &amp; Montepare, J. M. (2008). Social psychological face perception: Why appearance matters. <em>Social and Personality Psychology Compass</em>, <em>2</em>(3), 1497–1517.
</div>
<div id="ref-R-kableExtra" class="csl-entry">
Zhu, H. (2021). <em>kableExtra: Construct complex table with ’kable’ and pipe syntax</em>. <a href="https://CRAN.R-project.org/package=kableExtra">https://CRAN.R-project.org/package=kableExtra</a>
</div>
<div id="ref-zhu2017unpaired" class="csl-entry">
Zhu, J.-Y., Park, T., Isola, P., &amp; Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 2223–2232.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>All web search figures are from Google Scholar in May 2022.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
